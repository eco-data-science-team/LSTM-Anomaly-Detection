{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import keras\n",
    "from keras.optimizers import *\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy import array\n",
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (20.0, 10.0) # to make any matplotlib plot automatically a 20x10\n",
    "\n",
    "\n",
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config/lstmconfig.ini')\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "eco_tools_path = config['SETUP']['eco_tools_path']\n",
    "sys.path.append(eco_tools_path)\n",
    "from ecotools.pi_client import pi_client\n",
    "pc = pi_client(root = 'readonly')\n",
    "\n",
    "callbacks_list = [EarlyStopping(monitor='val_loss', patience=5, min_delta=0.001, verbose=2)]\n",
    "#sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_name = config['PI']['point_name']\n",
    "start = config['PI']['start']\n",
    "end = config['PI']['end']\n",
    "interval = config['PI']['interval']\n",
    "calculation = config['PI']['calculation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mulitvariable_df(data):\n",
    "    data.rename(columns={'aiTIT4045':'OAT'}, inplace=True)\n",
    "\n",
    "    data[\"cdd\"] = data.OAT - 65.0\n",
    "    data.loc[data.cdd < 0, \"cdd\"] = 0\n",
    "    data[\"hdd\"] = 65.0 - data.OAT\n",
    "    data.loc[data.hdd < 0, 'hdd'] = 0\n",
    "    data[\"cdd2\"] = data.cdd**2\n",
    "    data[\"hdd2\"] = data.hdd**2\n",
    "\n",
    "    data2 = data.copy()\n",
    "    ##FIX: if the dataframe passed does not have a month that the origial LSTM trained on, it wont be able\n",
    "    ##to be used to predict!\n",
    "    data2[\"MONTH\"]= data2.index.month\n",
    "    data2[\"MONTH\"] = data2[\"MONTH\"].astype('category')\n",
    "    data2[\"TOD\"] = data2.index.hour\n",
    "    data2[\"TOD\"] = data2[\"TOD\"].astype('category')\n",
    "    data2[\"DOW\"] = data2.index.weekday\n",
    "    data2[\"DOW\"] = data2[\"DOW\"].astype('category')\n",
    "    print(f\"data2: {data2['MONTH'].unique()}\")\n",
    "    ### Create dummy variables\n",
    "    l3 = [\"MONTH\",\"TOD\",\"DOW\"]#,“WEEK”]#,“DOY”]\n",
    "    data2 = pd.get_dummies(data=data2, columns=l3, drop_first=True)\n",
    " \n",
    "    ### Create Weekend flag\n",
    "    data2[\"WEEKEND\"] = 0\n",
    "    data2.loc[(data2.DOW_5 == 1) | (data2.DOW_6 == 1), 'WEEKEND'] = 1\n",
    "    \n",
    "    data2[\"shift1\"] = data2.iloc[:,0].shift(2)\n",
    "\n",
    "    data2[\"rolling24_mean\"] = data2.iloc[:,0].rolling('24h').mean()\n",
    "    data2[\"rolling24_max\"] = data2.iloc[:,0].rolling('24h').max()\n",
    "    data2[\"rolling24_min\"] = data2.iloc[:,0].rolling('24h').min()\n",
    "   \n",
    "    data2.dropna(inplace=True)\n",
    "    \n",
    "    return data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_list = [point_name, 'aiTIT4045']\n",
    "df = pc.get_stream_by_point(point_list, start = start, end = end, calculation = calculation, interval= interval)\n",
    "df = df.dropna(how='any')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_train_data(df):\n",
    "    #mask1 = (df[point_name] > 2400 )& (df.index.year < 2019)\n",
    "    mask1 = (df[point_name] > 2400 )\n",
    "    df1 = df.loc[mask1]\n",
    "    mask2 = (df.index.year>=2019)\n",
    "    df2 = df.loc[mask2]\n",
    "    return df1, df2\n",
    "    #return pd.concat([df1,df2])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, df1 = clean_train_data(df)\n",
    "print(f\"df: {df.shape} \\n df1: {df1.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = create_mulitvariable_df(df)\n",
    "df1 = create_mulitvariable_df(df1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.setdiff1d(df.columns,df1.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_keras(X, y):\n",
    "    # normalize the dataset\n",
    "    scaler_x = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler_y = MinMaxScaler((0, 1))\n",
    "    X = scaler_x.fit_transform(X)\n",
    "    y = scaler_y.fit_transform(np.array(y).reshape((-1,1)))\n",
    "    # split into train and test sets\n",
    "    train_size = int(len(X) * 0.7)\n",
    "    test_size = len(X) - train_size\n",
    "    X_train, X_test = X[0:train_size], X[test_size:len(X)]\n",
    "    y_train, y_test = y[0:train_size], y[test_size:len(y)]\n",
    "    return X_train, X_test, y_train, y_test, scaler_x, scaler_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[point_name]\n",
    "X = df.drop(columns=point_name)\n",
    "y1 = df1[point_name]\n",
    "X1 = df1.drop(columns=point_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, scaler_x, scaler_y = scale_keras(X, y)\n",
    "X1_train, X1_test, y1_train, y1_test, scaler_x1, scaler_y1 = scale_keras(X1, y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape input to be [samples, time steps, features]\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "X1_train = np.reshape(X1_train, (X1_train.shape[0], 1, X1_train.shape[1]))\n",
    "X1_test = np.reshape(X1_test, (X1_test.shape[0], 1, X1_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = DataFrame()\n",
    "val = DataFrame()\n",
    "np.random.seed(42)\n",
    "for i in range(1):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences = True))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer = Adam(lr = 0.001), loss = 'mean_squared_error')\n",
    "    #X,y = get_train()\n",
    "    #valX, valY = get_val()\n",
    "    # fit model\n",
    "    history = model.fit(X_train, y_train, epochs = 400, validation_split = 0.3, shuffle = False)\n",
    "    # story history\n",
    "    train[str(i)] = history.history['loss']\n",
    "    val[str(i)] = history.history['val_loss']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "figure(num=None, figsize=(15, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.plot(train, color='blue', label='train')\n",
    "plt.plot(val, color='orange', label='validation')\n",
    "plt.title('model train vs validation loss\\n 2 Layers-100 Neurons')\n",
    "plt.ylabel('loss (mse)')\n",
    "plt.xlabel('epoch')\n",
    "#plt.savefig('new_300_epochs.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "plt.plot(y_train, linewidth=1, color='green')\n",
    "plt.plot(y_test, color='lightgreen', linewidth=1)\n",
    "plt.title(\"Training and Test Split\")\n",
    "plt.legend(['Training','Test'])\n",
    "\n",
    "pred_train = history.model.predict(X_train)\n",
    "pred_train_inv = scaler_y.inverse_transform(pred_train.reshape(-1,1))\n",
    "pred_test = history.model.predict(X_test)\n",
    "pred_test_inv = scaler_y.inverse_transform(pred_test.reshape(-1,1))\n",
    "\n",
    "plt.figure(figsize=(18,2))\n",
    "training_comparison = pd.DataFrame({\"Actual\":scaler_y.inverse_transform(y_train).reshape((-1,)),\n",
    "                                   \"Modeled\":pred_train_inv.reshape((-1,))}, index=range(len(y_train)))\n",
    "training_comparison.sort_index().plot(figsize=(18,3),\n",
    "                         title=\"Training Data vs Model Prediction\",\n",
    "                         linewidth=1,\n",
    "                         color=['blue','red'])\n",
    " \n",
    "plt.figure(figsize=(18,2))\n",
    "test_comparison = pd.DataFrame({\"Actual\":scaler_y.inverse_transform(y_test).reshape((-1,)),\n",
    "                                   \"Modeled\":pred_test_inv.reshape((-1,))}, index=range(len(y_test)))\n",
    "test_comparison.sort_index().plot(figsize=(18,3),\n",
    "                     title=\"Test Data vs Model Prediction\",\n",
    "                     linewidth=1,\n",
    "                     colormap='winter')\n",
    "\n",
    "r2_train = r2_score(scaler_y.inverse_transform(y_train).reshape((-1,)), pred_train_inv.reshape(-1,1))\n",
    "print(f\"R2 of train = {round(r2_train,3)}\")\n",
    "r2_test = r2_score(scaler_y.inverse_transform(y_test).reshape((-1,)), pred_test_inv.reshape(-1,1))\n",
    "print(f\"R2 of test = {round(r2_test,3)}\")\n",
    "print()\n",
    "rmse_train = math.sqrt(mean_squared_error(scaler_y.inverse_transform(y_train).reshape((-1,)), pred_train_inv.reshape(-1,1)))\n",
    "print(f\"RMSE of train = {round(rmse_train,3)}\")\n",
    "rmse_test = math.sqrt(mean_squared_error(scaler_y.inverse_transform(y_test).reshape((-1,)), pred_test_inv.reshape(-1,1)))\n",
    "print(f\"RMSE of test = {round(rmse_test,3)}\")\n",
    "print()\n",
    "mae_train = np.median((pred_train_inv.reshape(-1,1) - scaler_y.inverse_transform(y_train).reshape((-1,))))\n",
    "print(f\"MAE of train = {round(mae_train,3)}\")\n",
    "mae_test = np.median((pred_test_inv.reshape(-1,1) - scaler_y.inverse_transform(y_test).reshape((-1,))))\n",
    "print(f\"MAE of test = {round(mae_test,3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
